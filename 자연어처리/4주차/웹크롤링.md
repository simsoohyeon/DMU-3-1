## 모듈 임포트 
```
import time # 딜레이 위한 내장 모듈 
import pandas as pd # 데이터를 테이블 형태로 저장하고 CSV로 내보내는데 사용 
from selenium import webdriver # 웹 브라우저를 자동으로 조작하는 라이브러리
from selenium.webdriver.common.by import By 
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup # HTML 코드 파싱해서 필요한 정보 쉽게 추출하는 라이브러리
```

## 검색 URL 생성 함수
```
def generate_page_url(keyword): # 검색 키워드 keyword 받아서 DBpia 검색 url 만들어줌 
    return f"https://www.dbpia.co.kr/search/topSearch?searchOption=all&query={keyword}"
```
## 크롬 드라이버 실행 
```
driver = webdriver.Chrome() # 셀레니움이 크롬 브라우저를 띄움, chromedriver 설치 필요
```
## try-finally 구조 시작
```
try: # 오류가 발생해도 마지막에 브라우저 종료하도록 finally 함께 사용 
    search_keyword = "딥러닝" # 검색할 키워드설정
    max_pages = 210 # 몇 페이지까지 크롤링할지 지정 
    url = generate_page_url(search_keyword) # 앞에서 정의한 함수로 검색 URL 만들고 접속
    driver.get(url)
    checkbox_ids = [...] # 디비피아에서는 등재된 논문 필터링을 위해 체크박스 클릭 
    for checkbox_id in checkbox_ids: # 체크박스가 보일 때까지 기다렸다가 클릭 
        ...
    time.sleep(4)
    papers = [] # 논문 정보 저장할 리스트  
    detail_page_links = [] # 초록이 없는 논문은 상세 페이지에서 다시 가져오기 위해 detail_page_links에 저장
    for current_page in range(1, max_pages + 1): # 1페이지부터 설정한 페이지 수까지 반복해서 논문 데이터 수집 
        WebDriverWait(driver, 10).until(...) # 로딩 대기 및 페이지 소스 파싱
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        # 검색 결과가 나타날 때까지 기다린 후 BeautifulSoup으로 HTML을 파싱
        for n in range(1, 22): # 한 페이지에 최대 21개의 논문이 있다고 가정하고 반복 
        # 제목, 초록, 저자, 키워드, 날짜 추출
        # select_one, find_all, get_text(strip=True)를 사용해 불필요한 공백 제거
            if not abstract and full_detail_page_url:  # 초록이 없을 경우 상세 페이지 url 저장
                detail_page_links.append((title, full_detail_page_url))
            papers.append({...}) # 논문 리스트에 추가 
        if current_page % 10 == 0: # 디비피아는 페이징 버튼이 10개 단위로 분기
        # 10페이지마다 다음 버튼으로 넘어가고, 그 외에는 해당 페이지 숫자를 눌러 이동 
            ...
        else:
            ...
    for title, detail_url in detail_page_links: # 초록 없는 논문 상세 페이지 크롤링
    # driver.get()으로 상세페이지 접속
    # 초록(abstract) HTML 요소가 로드될 때까지 기다렸다가 크롤링
            for paper in papers: # 기존 데이터에서 해당 논문 찾아 초록 업데이트 
                if paper["제목"] == title:
                    paper["초록"] = abstract
                    break
    df = pd.DataFrame(papers) # pandas로 테이블화 
    df = df[~((df["제목"] == "제목 없음") & ...)] # 제목, 초록, 저자 모두 없는 레코드 제거 
    file_name = f"dbpia_papers_{search_keyword}_pages_{max_pages}.csv" # csv로 저장
    df.to_csv(file_name, index=False, encoding="utf-8-sig") 
finally: 
    driver.quit() # 브라우저 종료 

```
